<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://psouranis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://psouranis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-12T07:12:48+00:00</updated><id>https://psouranis.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Pruning techniques - Optimizing machine learning models Part 2</title><link href="https://psouranis.github.io/blog/2025/pruning-techniques-part2/" rel="alternate" type="text/html" title="Pruning techniques - Optimizing machine learning models Part 2"/><published>2025-03-11T12:40:16+00:00</published><updated>2025-03-11T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/pruning-techniques-part2</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/pruning-techniques-part2/"><![CDATA[<p>This article is the continuation of the previous one around some base pruning techniques <a href="https://psouranis.github.io/blog/2025/pruning-techniques/">[1]</a>.</p>]]></content><author><name></name></author><category term="Optimizations"/><category term="Inference"/><category term="Optimizations"/><category term="Performance"/><category term="ML"/><summary type="html"><![CDATA[Pruning Techniques Part 2]]></summary></entry><entry><title type="html">Pruning techniques - Optimizing machine learning models Part 1</title><link href="https://psouranis.github.io/blog/2025/pruning-techniques/" rel="alternate" type="text/html" title="Pruning techniques - Optimizing machine learning models Part 1"/><published>2025-03-04T12:40:16+00:00</published><updated>2025-03-04T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/pruning-techniques</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/pruning-techniques/"><![CDATA[<p>In this article, we are going to focus in optimizing machine learning models after training ends.</p> <h3 id="pruning">Pruning</h3> <div style="text-align: center;"> <img src="/assets/post_images/pruned_tree.png"/> </div> <p>Pruning is the practice of removing parameters (which may entail removing individual parameters, or parameters in groups such as by neurons) from an existing artificial neural networks.[1] The goal of this process is to maintain accuracy of the network while increasing its efficiency. This can be done to reduce the computational resources required to run the neural network <a href="https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)">[1]</a>.</p> <div style="text-align: center;"> <img src="/assets/post_images/pruned_network.png"/> </div> <p>There are 2 distinctions in pruning: <strong>Global</strong> and <strong>Local</strong> pruning.</p> <h4 id="global-vs-local-pruning">Global vs Local pruning</h4> <p>The difference between global and local pruning lies in whether structures are removed from a subset or all available structures of a network. A major limitation of local pruning is that setting a pre-defined prune ratio for each layer can be complex and lead to sub-optimal sparsity. To simplify, local pruning often uses a consistent prune ratio across layers. In contrast, global pruning automatically generates a varying prune ratio for each layer. However, global pruning poses great challenges (particularly for LLMs), due to significant variations in layer magnitudes. For instance, some outlier features may have magnitudes up to 20 times larger than others, leading to incomparability issues.</p> <p>In the following post, we are going to focus on <strong>Local</strong> pruning since it poses an advantage compared to global pruning.</p> <p>Letâ€™s review some of the most common techniques for pruning:</p> <h4 id="random-unstructured">Random Unstructured</h4> <p>As the name suggest, in random unstructured pruning we prune each tensor of our network by randomly removing weights (connections between neurons) of its layers. In PyTorch the most common way to apply random unstructured pruning is via the <code class="language-plaintext highlighter-rouge">torch.nn.utils.random_unstrucuted</code>.</p> <p>We can apply it to each layer of our network like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_unstructured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>In the script above there are couple of things happening:</p> <p>First, we iterate through each layer of our network using the <code class="language-plaintext highlighter-rouge">.named_modules()</code> method. Then we pass to the <code class="language-plaintext highlighter-rouge">random_unstructured</code> function the <code class="language-plaintext highlighter-rouge">module</code>, the <code class="language-plaintext highlighter-rouge">name</code> and the <code class="language-plaintext highlighter-rouge">amount</code> parameters. We do that because we want to target the <code class="language-plaintext highlighter-rouge">weight</code> attribute of our <code class="language-plaintext highlighter-rouge">Module</code> which contains the weights (it could also be applied to <code class="language-plaintext highlighter-rouge">bias</code> as well).</p> <p>Finally, we apply the <code class="language-plaintext highlighter-rouge">remove</code> method to the <code class="language-plaintext highlighter-rouge">module</code> because the <code class="language-plaintext highlighter-rouge">random_unstructured</code> method applies pruning to each tensor but also keeps the original weights in the attribute <code class="language-plaintext highlighter-rouge">weight_orig</code>. If we want to make the pruning permanent and discard the original weights, we have to remove them by using the <code class="language-plaintext highlighter-rouge">remove</code> function as we did above.</p> <blockquote> <p>ðŸ’¡ <strong>Important note</strong> <br/><br/> When we apply pruning through this method, essentially what we are doing is applying a random mask of zeroes and ones on every layer in order to cancel them out. You can actually access those masks via the <code class="language-plaintext highlighter-rouge">weight_mask</code> attribute. <br/><br/> It is important to make this clear since masking a layer does nothing to the network parameters and hence its size or to how many operations does. <br/><br/> The total number of parameters remains the same (it actually increases because we have to also store the masks along with the actual weights) but we will see later how we can actually prune the layer and indeed remove parameters from it.</p> </blockquote> <ul> <li><em>Pros of Random Unstructured Pruning</em> <ul> <li>Simple to implement.</li> <li>Extremely flexible, as it can prune any element in any dimension.</li> <li>Generally has better performance in terms of accuracy. <br/><br/></li> </ul> </li> <li><em>Cons of Random Unstructured Pruning</em> <ul> <li>Pruning is not guaranteed to be uniform across the tensor.</li> <li>Pruning is not guaranteed to be uniform across the dimensions.</li> <li>Very hard to accelerate with hardware acceleration.</li> </ul> </li> </ul> <h4 id="random-structured">Random Structured</h4> <p>Structured pruning is very similar with unstructured pruning only in this case we donâ€™t only remove weights (like previously) but we remove entire neurons (or entire structural components like filters, channels in Convolutional networks). This type of pruning tends to reduce accuracy of the model but it has the benefit that it achieves better performance in terms of inference speed. In this case by cutting entire rows or columns of our tensors we dramatically reduce the number of calculations that need to be executed.</p> <div style="text-align: center;"> <img src="/assets/post_images/structured_2d_pruning.png" style="max-width: 50%"/> </div> <p>In PyTorch to apply random structured pruning, we can do so via the <code class="language-plaintext highlighter-rouge">torch.nn.utils.random_structured</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_structured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_structured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td></tr></tbody></table></code></pre></div></div> <ul> <li><em>Pros of Random Structured Pruning</em> <ul> <li>Simple to implement.</li> <li>Can be used to prune any dimension of a tensor.</li> <li>Easier to accelerate (just a smaller matrix)</li> <li>Generally it offers lower accuracy but better performance in terms of inference speed. <br/><br/></li> </ul> </li> <li><em>Cons of Random Sstructured Pruning</em> <ul> <li>Less flexible as it prunes entire rows or columns of tensors</li> <li>Pruning is not guaranteed to be uniform across the tensor.</li> </ul> </li> </ul> <h4 id="ln-unstructured">Ln Unstructured</h4> <p>In \(\ell_{n}\) unstructured, we prune each tensor of our network by removing the lowest weights according to the \(\ell_{n}\) norm. Letâ€™s see the following example using the \(\ell_1\) norm to calculate the importance of the tensor weights:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_unstructured.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>We start with a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> that has no pruned weights. We then apply <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning to prune the lowest weights. To do so we first calculate the \(\ell_{1}\) norm of the tensor:</p> \[\ell_{1}(t) = \sum_{i=1}^{n} |w_{i}|\] <p>Where \(n\) is the total number of elements in the Tensor. Then we remove the \(40\%\) of the lowest weights of our Tensor.</p> <p>Similar with random unstructured pruning, \(\ell_n\) unstructured purning shares the same pros and cons.</p> <ul> <li><em>Pros of Ln Unstructured Pruning</em> <ul> <li>Simple to implement.</li> <li>Extremely flexible, as it can prune any element in any dimension.</li> <li>Generally has better performance in terms of accuracy compared to Ln Structured Pruning.</li> <li>Focuses more in the importance of weights compared to Random Unstructured and hence can result in higher performance compared to Random Unstructured with higher compression ratio. <br/><br/></li> </ul> </li> <li><em>Cons of Ln Unstructured Pruning</em> <ul> <li>Pruning is not guaranteed to be uniform across the dimensions.</li> <li>Very hard to accelerate with hardware acceleration.</li> <li>As the pruning ratio increases, it can result in suboptimal networks since it focuses more on the weights with lower importance and the distribution of weights will change dramatically.</li> </ul> </li> </ul> <h4 id="ln-structured">Ln Structured</h4> <p>Finally in \(\ell_{n}\) structured, we prune across the dimensions of our tensors, dropping weights with the lowest \(\ell_{n}\) norm. Like previously, we will use the \(\ell_1\) norm in order to calculate the importance.</p> <p>It is interesting to see what happens when we prune across the dimensions of a weight matrix. Letâ€™s take as an example a Linear layer and see what happens for <code class="language-plaintext highlighter-rouge">dim=0</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim0.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Now letâ€™s take the case where <code class="language-plaintext highlighter-rouge">dim=1</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim1.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Suppose that we have a N-layer network. We can see that in the first case where <code class="language-plaintext highlighter-rouge">dim=0</code> for each layer \(L\) we prune the neurons of the \(L\) layer, whereas if <code class="language-plaintext highlighter-rouge">dim=1</code> we prune the neurons of the \(L+1\) layer.</p> <ul> <li><em>Pros of Ln Structured Pruning</em> <ul> <li>Simple to implement.</li> <li>Can be used to prune any dimension of a tensor.</li> <li>Easier to accelerate (just a smaller matrix).</li> <li>It can provide easy quick wins to get a compressed model.</li> <li>Provides very high compression ratio. <br/><br/></li> </ul> </li> <li><em>Cons of Ln Structured Pruning</em> <ul> <li>Less flexible as it prunes entire rows or columns of tensors.</li> <li>Pruning is not guaranteed to be uniform across the tensor.</li> <li>It is more aggressive than Ln Unstructured and should be not used with high pruning ratios.</li> </ul> </li> </ul> <p>In the rest of the post, we will explore each of those techniques on the ImageNet validation dataset (50000) by using as a base model the Vision Transformer model that you can find <a href="https://huggingface.co/google/vit-base-patch16-224">[here]</a></p> <h4 id="results-in-imagenet1000-with-vit">Results in ImageNet1000 with VIT</h4> <p>After all is said and done, its now time to evaluate the different methods described above in the ImageNet dataset using as our base model the VisionTransformer. You can find the code that was used to run those experiments here: <a href="https://github.com/psouranis/pruning">link</a>.</p> <p>We pruned each layer of the network equally (with the same pruning ratio in each iteration) except the <code class="language-plaintext highlighter-rouge">LayerNorm</code> layers.</p> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(2, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/imagenet1000/pruning_results_random_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_random_structured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_l1_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_ln_structured.png" alt="Image 1" style="width: 100%; height: auto;"/> </div> <p>As we can see from the results above, our best compression-ratio / accuracy comes from the <code class="language-plaintext highlighter-rouge">l1_unstructured</code> method which is somewhat expected since it removes weights with the lowest importance. As we also mentioned above, <code class="language-plaintext highlighter-rouge">ln_structured</code> is extremely aggressive and immediatelly drives our accuracy close to 20% which is not acceptable.</p> <p>Finally we can try to recover some of our lost accuracy by finetuning our model. Letâ€™s see if we can recover the accuracy for the case where pruning ratio is 0.5 and for <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning method.</p> <h4 id="finetuning-with-pruning">Finetuning with pruning</h4> <p>In our experiments, we finetuned the pruned model in order to achieve some of the lost accuracy (around 71%). So, we managed to reduce our model size by 50% and lose only 12.5% of its initial accuracy.</p> <h4 id="summary">Summary</h4> <p>Experimental results using a Vision Transformer on ImageNet-1000 indicate that L1 Unstructured Pruning achieves the best balance between compression ratio and accuracy.<br/> Ln Structured Pruning was found to be too aggressive, significantly reducing accuracy. Finetuning a pruned model can recover some of the lost accuracy, as demonstrated with L1 Unstructured pruning, achieving a 50% model size reduction with a limited accuracy drop after finetuning.</p> <p>You can find the code available here: <a href="https://github.com/psouranis/pruning">Github</a></p>]]></content><author><name></name></author><category term="Optimizations"/><category term="Inference"/><category term="Optimizations"/><category term="Performance"/><category term="ML"/><summary type="html"><![CDATA[Pruning Techniques]]></summary></entry><entry><title type="html">Tail Call Optimization</title><link href="https://psouranis.github.io/blog/2025/tail-call-optimization/" rel="alternate" type="text/html" title="Tail Call Optimization"/><published>2025-02-14T12:40:16+00:00</published><updated>2025-02-14T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/tail-call-optimization</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/tail-call-optimization/"><![CDATA[<h3 id="tail-recursive-methods">Tail Recursive Methods</h3> <p>In this post we will talk about optimizations in C and specifically about Tail Call Optimization. But letâ€™s start from defining what is a tail recursion.</p> <p>A recursive method is referred to as tail-recursive <em>when the recursive call is the last thing executed by the that method</em>, otherwise, itâ€™s known as <em>head recursive</em></p> <p>Here is a simple example of a <em>tail recursive method</em>:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">printNumbers</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>	
	<span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
	<span class="n">printNumbers</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Notice in this function how the last thing executed by the method is a recrusive call, making it a <em>tail recursive method</em>. Letâ€™s take another example now, the factorial function. The factorial function can be implemented as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Is this a tail recursive method? On the first look someone would say yes but if we were to look more closely we would see that the function can be written as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>We realize that the last executed statement by this method is the multiplication and not the recursion meaning that the method we previously implemented is not <em>tail recursive</em> but rather <em>head recursive</em>. So letâ€™s try first to transform it to a tail recursive method and then discuss itâ€™s benefits.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">return</span> <span class="n">factorialTail</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">factorialTail</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="n">x</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Notice that now we have 2 methods instead of 1 (<code class="language-plaintext highlighter-rouge">factorial</code> and <code class="language-plaintext highlighter-rouge">factorialTail</code>). The original method is turned to a helper method and is now called with an initial value. This initial value, in most cases, will be the return value of the stop condition we had in the original recursive call.</p> <p>The next thing you will notice, <em>is how we included the multiplication</em> which was the operation executed after the recursive call, inside the the recursion itself as an <strong>accumulator</strong> and it is the final result of this <strong>accumulator</strong> that is being returned to the user when the calculation is finished.</p> <p>In a tail recusrive method, <em>the result of the stop condition is actually the result of the whole recursion</em> because that is what it will be returned by all child calls all the way to the parent and initial call.</p> <p>Letâ€™s take another example as well, the Fibonnaci sequence:</p> \[\text{Fibonacci}(n) = \text{Fibonnaci}(n-1) + \text{Fibonnaci}(n-2)\] <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">fibonnaci</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">fibonnaci</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fibonnaci</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Can be transformed to a tail recursive method as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">fibonnaci</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">return</span> <span class="n">fibonnaciTail</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">fibonnaciTail</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="n">a</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="n">b</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">fibonnaci_</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Similar to the factorial one, the original method is broken into two, the caller and the helper one. The caller method is using as initial values the return values of the stop-conditions we previously had. The result or the addition is now accumulated inside the recursive call and because the recursion is called on the previous value of our current value as well in the initial method â€“ we need to keep track the previous value of the initial call using an additional parameter.</p> <h3 id="continuation-passing-style">Continuation Passing Style</h3> <ol> <li> <p>The continuation passing style (passing accumulators as parameter values) might be the <strong>only generic way</strong> that allows you to transform the method into a form that uses <strong>only-tail calls</strong>.</p> </li> <li> <p>Not all logics can be made tail-recursive, non-linear recursions for instance maintain a variable somewhere and that somewhere is the <strong>Stack Memory</strong>.</p> </li> </ol> <h3 id="stack-visualization">Stack Visualization</h3> <p>Take a look at how the stack frame with tail call optimization and without.</p> <div style="text-align: center;"> <img src="/assets/post_images/tco.png"/> </div> <p>When <code class="language-plaintext highlighter-rouge">factorial(5)</code> letâ€™s say is called without tail call optimization â€” these calls will be respectively added to the stack, and we wonâ€™t be able to start popping from this stack before we reach the stop condition of the recursion.</p> <p>When that happens the values we have will start to get replaced and the stack calls will be popped.</p> \[\text{factorial}(5) \rightarrow \text{factorial}(4) \rightarrow \text{factorial}(3) \rightarrow \text{factorial}(2) \rightarrow \text{factorial}(1)\] <p>However, if we try this on the tail recursive method, the first call will be the call to the factorial method <code class="language-plaintext highlighter-rouge">factorial</code> and the second call will be the call to the <code class="language-plaintext highlighter-rouge">factorialTail</code>:</p> \[\text{factorial}(5) \rightarrow \text{factorialTail}(5,1)\] <p>When <code class="language-plaintext highlighter-rouge">factorial(4)</code> is called, because no variables are required to be stored in the frame left by the parent call of the same recursive method, and because both of these calls will have the same return value â€” then this same frame will be used to store the new call details:</p> \[\text{factorial}(5) \rightarrow \text{factorialTail}(1, 120)\] <p>Now, if we go back and compare the stacks corresponding to the recursive methods at their maximum depths, we will see that the space complexity of recursion was reduced from \(O(n)\) to \(O(1)\) thanks to tail-call elimination.</p> <div style="text-align: center;"> <img src="/assets/post_images/tco_complexity.png"/> </div> <h3 id="summary">Summary</h3> <h4 id="what-are-the-benefits-ot-tail-recursion-methods">What are the benefits ot Tail Recursion Methods?</h4> <ul> <li>Tail recursive methods are <strong>optimized</strong> by certain compilers. These compilers usually execute calls with the help of a stack.</li> <li>In case of tail-recursive methods, there is no need to reserve a place in the stack for the recursive call because there is nothing left to do in the current method and we wonâ€™t be returning to the parent call.</li> <li>The space complexity reduces from \(O(n)\) to \(O(1).\)</li> </ul>]]></content><author><name></name></author><category term="Optimizations"/><category term="C"/><category term="Optimizations"/><category term="Performance"/><summary type="html"><![CDATA[Tail Call Optimization]]></summary></entry></feed>