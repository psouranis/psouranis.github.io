<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://psouranis.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://psouranis.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-24T15:50:55+00:00</updated><id>https://psouranis.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">__getitem__ vs __getitems__. A nice trick for Pytorch</title><link href="https://psouranis.github.io/blog/2025/getitem/" rel="alternate" type="text/html" title="__getitem__ vs __getitems__. A nice trick for Pytorch"/><published>2025-05-15T12:40:16+00:00</published><updated>2025-05-15T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/getitem</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/getitem/"><![CDATA[<p><br/></p> <div style="text-align: center;"> <img src="/assets/post_images/getitem/getitem.png" style="width: 50%; height: auto;"/> </div> <p><br/></p> <p>This post is about one curious thing we discovered when we were dealing Pytorch’s <code class="language-plaintext highlighter-rouge">DataLoader</code>. Imagine the following scenario. Let’s say that you want to train an LSTM model for example on some embeddings. What we would have would be the following:</p> \[x_\text{sample} = (x_{0}, x_{1}, \ldots, x_{k}), \text{where } k=\text{window size} \text{ and } x_{i} \in \mathbf{R}^{d} \text{ where } d=\text{embedding dimension}\] <p>In Python we could represent it very easily like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">numpy.lib.stride_tricks</span> <span class="kn">import</span> <span class="n">sliding_window_view</span>
<span class="n">window_shape</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># How long our window will be
</span>
<span class="n">windows</span> <span class="o">=</span> <span class="nf">sliding_window_view</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">window_shape</span><span class="o">=</span><span class="n">window_shape</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">windows</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (91, 10)
</span><span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (100, 384)
</span><span class="nf">print</span><span class="p">(</span><span class="n">windows</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="c1">#[[ 0  1  2  3  4  5  6  7  8  9]
# [ 1  2  3  4  5  6  7  8  9 10]]
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>Imagine as a concrete example that each index can be a word and each word has its unique embedding.</p> <blockquote> <p><em>note</em></p> <p>We are not creating an entire dataset of shape (91, 10, 384) because we want to cache the embeddings and only index them when we are retrieving an example in order to avoid to be out of memory. This way we have a total memory consuption of 91 * 10 * 2 = 1820 and 384 * 100 * 8 = 3072, 309020 bytes so roughly 309 MBs. Instead if we create the whole embedding dataset we would have 91 * 10 * 384 * 8 = 2795520 bytes so roughly 3 GBs.</p> </blockquote> <p>Now let’s say that we want to create a Torch <code class="language-plaintext highlighter-rouge">Dataset</code> in order to retrieve samples from this training set. What we would normally do is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">windows</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span>
    <span class="n">self</span><span class="p">.</span><span class="n">windows</span> <span class="o">=</span> <span class="n">windows</span>

  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">)</span>

<span class="n">nn_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">nn_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">windows</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># torch.Size([10, 384])
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>Alright so far so good, so now our next step would be to create a <code class="language-plaintext highlighter-rouge">DataLoader</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Let’s see how fast we can iterate through our <code class="language-plaintext highlighter-rouge">Dataset</code> with our <code class="language-plaintext highlighter-rouge">DataLoader</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">time</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="nf">print</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
<span class="c1"># 0.0038449764251708984
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>Now as you can see in order to iterate through our whole dataset which consists of 91 examples, we needed with batch size of 8 samples around 3 miliseconds. That’s quite fast but let’s see if we can do anything better.</p> <p>If you check the documentation and the source code of the <code class="language-plaintext highlighter-rouge">Dataset</code> class you will end up to the class <code class="language-plaintext highlighter-rouge">_MapDatasetFetcher</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">_MapDatasetFetcher</span><span class="p">(</span><span class="n">_BaseDatasetFetcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">possibly_batched_index</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">auto_collation</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span> <span class="sh">"</span><span class="s">__getitems__</span><span class="sh">"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">__getitems__</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="nf">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>You can spot the devil in the details right?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>This line right here, means that our Dataset collects each window embedding seperately and then it concats them with the help of the <code class="language-plaintext highlighter-rouge">self.collate_fn</code> function. So this is all fine and good but can’t we collect all the indexes at once and make a single call to <code class="language-plaintext highlighter-rouge">__getitem__</code> ? After all the <code class="language-plaintext highlighter-rouge">nn.Embedding</code> supports all sorts of types of indexing and its optimized at its core to make such calls.</p> <p>This is where <code class="language-plaintext highlighter-rouge">__getitems__</code> comes in play. This little function which for some reason is not documented in Pytorch’s documentation is extremely helpful and serves exactly this purpose. Instead of making multiple calls to <code class="language-plaintext highlighter-rouge">__getitem__</code> and stack them in <code class="language-plaintext highlighter-rouge">collate_fn</code> we will make a single call to <code class="language-plaintext highlighter-rouge">__getitems__</code> after we have collected all indexes. And to implement it is very straightforward.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">windows</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span>
    <span class="n">self</span><span class="p">.</span><span class="n">windows</span> <span class="o">=</span> <span class="n">windows</span>

  <span class="k">def</span> <span class="nf">__getitems__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">idxs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">[</span><span class="n">idxs</span><span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">)</span>

<span class="n">nn_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">nn_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">windows</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>What we did is to replace the <code class="language-plaintext highlighter-rouge">__getitem__</code> with the <code class="language-plaintext highlighter-rouge">__getitems__</code>. Now let’s see if we can have any improvement.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="nf">print</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
<span class="c1"># 0.0024793148040771484
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>As you will notice here, we also replaced the default collate function which uses <code class="language-plaintext highlighter-rouge">torch.stack</code> to collect all the examples with just <code class="language-plaintext highlighter-rouge">collate_fn=lambda x: x</code> since each example now is a batch itself!.</p> <p>The difference as you can see is minor here but the results become more apparent once we increase the size of our window dataset. Let’s do that and see the difference</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">windows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">windows</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Here, we just repeated the rows.</p> <p>Now, with our first version, we get the following timing.</p> <ul> <li><code class="language-plaintext highlighter-rouge">__getitem__</code>: 1.2 seconds.</li> <li><code class="language-plaintext highlighter-rouge">__getitems__</code>: 0.6 seconds.</li> </ul> <p>So we are down by 50% by making a pretty small change in our code. So next time you will implement an Embedding dataset just remember that the devil is in the details and hidden <code class="language-plaintext highlighter-rouge">concats</code> can make a great difference in the performance.</p>]]></content><author><name></name></author><category term="Optimizations"/><category term="Inference"/><category term="Optimizations"/><category term="Performance"/><category term="ML"/><summary type="html"><![CDATA[Torch indexing trick]]></summary></entry><entry><title type="html">Pruning techniques - Optimizing machine learning models Part 2</title><link href="https://psouranis.github.io/blog/2025/pruning-techniques-part2/" rel="alternate" type="text/html" title="Pruning techniques - Optimizing machine learning models Part 2"/><published>2025-03-11T12:40:16+00:00</published><updated>2025-03-11T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/pruning-techniques-part2</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/pruning-techniques-part2/"><![CDATA[<p>This article is the continuation of the previous one around some base pruning techniques <a href="https://psouranis.github.io/blog/2025/pruning-techniques/">[1]</a>.</p>]]></content><author><name></name></author><category term="Optimizations"/><category term="Inference"/><category term="Optimizations"/><category term="Performance"/><category term="ML"/><summary type="html"><![CDATA[Pruning Techniques Part 2]]></summary></entry><entry><title type="html">Pruning techniques - Optimizing machine learning models Part 1</title><link href="https://psouranis.github.io/blog/2025/pruning-techniques/" rel="alternate" type="text/html" title="Pruning techniques - Optimizing machine learning models Part 1"/><published>2025-03-04T12:40:16+00:00</published><updated>2025-03-04T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/pruning-techniques</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/pruning-techniques/"><![CDATA[<p>In this article, we are going to focus in optimizing machine learning models after training ends.</p> <h3 id="pruning">Pruning</h3> <p><br/></p> <div style="text-align: center;"> <img src="/assets/post_images/tree.webp" style="width: 50%; height: auto;"/> </div> <p><br/></p> <p>Pruning is the practice of removing parameters (which may entail removing individual parameters, or parameters in groups such as by neurons) from an existing artificial neural networks.[1] The goal of this process is to maintain accuracy of the network while increasing its efficiency. This can be done to reduce the computational resources required to run the neural network <a href="https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)">[1]</a>.</p> <div style="text-align: center;"> <img src="/assets/post_images/pruned_network.png"/> </div> <p>There are 2 distinctions in pruning: <strong>Global</strong> and <strong>Local</strong> pruning.</p> <h4 id="global-vs-local-pruning">Global vs Local pruning</h4> <p>The key distinction between local and global pruning lies in their scope: local methods remove connections within individual layers, while global methods consider the entire network. A significant drawback of local pruning is the difficulty in determining an optimal, layer-specific pruning ratio, often leading to a uniform ratio across all layers for simplicity. In contrast, global pruning dynamically assigns varying prune ratios per layer. However, this approach faces considerable hurdles, particularly with Large Language Models (LLMs), due to substantial disparities in layer magnitudes. For example, some features can be up to 20 times larger than others, creating issues with direct comparison during the pruning process.</p> <p>In the following post, we are going to focus on <strong>Local</strong> pruning since it poses an advantage compared to global pruning.</p> <h4 id="structured-vs-unstructured">Structured vs Unstructured</h4> <p>Structured pruning removes entire groups of connected parameters like neurons or filters, resulting in a smaller, more regular network that is generally easier for hardware to process efficiently. In contrast, unstructured pruning removes individual weights, leading to a sparse network with the same overall structure but with many zeroed connections, which can achieve higher compression but may require specialized hardware to fully realize speedups.</p> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(1, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/unstructured.png" style="width: 52%; height: auto;"/> <img src="/assets/post_images/structured_2d_pruning.png" style="width: 100%; height: auto;"/> </div> <p>Let’s review some of the most common techniques for pruning:</p> <h4 id="random-unstructured">Random unstructured</h4> <p>As the name suggests, random unstructured pruning involves randomly removing individual weights (the connections between neurons) within each layer of your network. In PyTorch, a common way to achieve this is using the <code class="language-plaintext highlighter-rouge">torch.nn.utils.prune.random_unstructured</code> function.</p> <p>You can apply it to each layer like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_unstructured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td></tr></tbody></table></code></pre></div></div> <p>In the provided script, we’re performing the following steps:</p> <ol> <li>We iterate through each layer of our network using <code class="language-plaintext highlighter-rouge">.named_modules()</code> to access both the layer’s name and the layer itself.</li> <li>For each layer, we apply the <code class="language-plaintext highlighter-rouge">random_unstructured</code> pruning function. This function requires three key arguments: the module (the layer), the name of the parameter to prune (in our case, <code class="language-plaintext highlighter-rouge">'weight'</code> to target the layer’s weights – you could also prune biases), and the amount of pruning.</li> <li>Crucially, <code class="language-plaintext highlighter-rouge">random_unstructured</code> doesn’t immediately delete weights. It masks them and stores the original weights in <code class="language-plaintext highlighter-rouge">weight_orig</code>. To finalize the pruning and remove the original weights, we then call the <code class="language-plaintext highlighter-rouge">remove()</code> method on the module.</li> </ol> <blockquote> <p>💡 Important note <br/><br/> When we apply pruning using this method, we’re essentially creating a binary mask (composed of zeros and ones) for each layer’s weight tensor. A zero in the mask indicates a weight that should be treated as zero during calculations, effectively “canceling it out” in the forward pass. You can inspect these masks directly through the <code class="language-plaintext highlighter-rouge">weight_mask</code> attribute of the pruned layer. <br/><br/> It’s crucial to understand that masking alone doesn’t reduce the actual number of parameters or the size of the network in memory, nor does it decrease the number of computations performed. The underlying weight values are still present. In fact, the total number of parameters might even increase slightly because we now have to store these masks alongside the original weights (especially in the case where finetuning is applied).</p> </blockquote> <ul> <li><strong>Pros of Random unstructured pruning</strong> <ul> <li><em>Ease of implementation</em>: This method is straightforward to implement, making it a good starting point for exploring network pruning techniques.</li> <li><em>High granularity and flexibility</em>: Random unstructured pruning offers the ultimate flexibility by allowing the removal of any individual weight connection within the network, regardless of its layer or position..</li> <li><em>Potential for high accuracy</em>: For a given level of sparsity, this method often achieves better or comparable accuracy compared to more structured pruning approaches.</li> </ul> </li> <li><strong>Cons of Random unstructured pruning</strong> <ul> <li><em>Sparsity distribution challenges</em>: The random nature of pruning doesn’t guarantee a uniform distribution of removed weights within a single layer’s tensor. This can lead to some parts of the layer being more sparse than others.</li> <li><em>Dimensional imbalance</em>: Similarly, the sparsity level might not be consistent across different dimensions (e.g., input vs. output channels) within a layer, potentially impacting the importance of certain features.</li> <li><em>Limited hardware acceleration</em>: A significant drawback is that the resulting sparse network has an irregular structure, making it very difficult to exploit the efficiencies of specialized hardware accelerators designed for structured sparsity patterns.</li> </ul> </li> </ul> <h4 id="random-structured">Random Structured</h4> <p>Structured pruning differs from unstructured pruning in that instead of removing individual weights, it eliminates entire structural units within the network. This could mean removing whole neurons or, in the context of Convolutional Neural Networks (CNNs), entire filters or channels. While this approach generally leads to a greater reduction in accuracy compared to unstructured pruning, it offers a significant advantage: improved inference speed. By removing entire rows or columns of weight tensors, we drastically reduce the number of computations required.</p> <p>In PyTorch, you can perform random structured pruning using the <code class="language-plaintext highlighter-rouge">torch.nn.utils.prune.random_structured</code> function.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_structured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_structured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td></tr></tbody></table></code></pre></div></div> <ul> <li><strong>Pros of Random structured pruning</strong> <ul> <li><em>Simplicity</em>: Randomly selecting entire rows, columns, or other structured units to prune is conceptually and often computationally simple to implement.</li> <li><em>Potential for dense substructures</em>: By removing entire structures, the remaining tensor might still be relatively dense, making it potentially easier to leverage existing efficient dense matrix multiplication (GEMM) implementations compared to highly sparse unstructured matrices.</li> <li><em>Parameter reduction</em>: It effectively reduces the number of parameters and computations required, leading to greater speedups in inference.</li> </ul> </li> <li><strong>Cons of Random structured pruning</strong> <ul> <li><em>Suboptimal pruning granularity</em>: Pruning at the level of entire structures (rows, columns) can be less fine-grained than unstructured pruning, potentially leading to a larger drop in accuracy for the same sparsity level.</li> <li><em>Limited flexibility in sparsity distribution</em>: The random selection of structures to prune might not result in an optimal distribution of sparsity across the tensor, potentially leaving important weights unpruned while removing less critical ones.</li> <li><em>Accuracy sensitivity</em>: The random nature of the pruning can lead to variability in the final accuracy depending on which specific structures were removed.</li> <li><em>Difficulty in leveraging structure</em>: Even though structured pruning offers us some structure in the resulting pruned layer, its difficult to drop the dimensions entirely and requires very careful stitching.</li> </ul> </li> </ul> <h4 id="l1-unstructured">L1 unstructured</h4> <p>Okay, continuing with the explanation of \(\ell_1\)​ unstructured pruning:</p> <p>In \(\ell_1\)​ unstructured pruning, the importance of each weight in a tensor is determined by its magnitude as measured by the \(\ell_1\)​ norm. We then prune the weights with the smallest importance (lowest norm values). This method aims to remove the weights that have the least impact on the network’s output.</p> <p>Let’s delve into an example using the \(\ell_1\)​ norm. The \(\ell_1\)​ norm of a single weight is simply its absolute value. Therefore, in \(\ell_1\)​ unstructured pruning, we would identify the weights with the smallest absolute values as the least important and remove them.</p> <p>Here’s how we might think about calculating the importance of tensor weights using the \(\ell_1​\) norm:</p> \[||w_{ij}||_1 = |w_{ij}|\] <p>We would then calculate this value for every weight in the tensor and prune those with the smallest magnitudes.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_unstructured.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>In the example above we start with a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> that has no pruned weights. We then apply <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning to prune the weights with the lowest \(ell_1\) norm. To do so we first calculate the <em>Importance</em> of the tensor:</p> \[\text{Importance} = |W|\] <p>Then we remove the \(40\%\) of the lowest weights of our Tensor.</p> <p>Similar with random unstructured pruning, \(\ell_1\) unstructured purning shares the same pros and cons.</p> <ul> <li><strong>Pros of L1 unstructured pruning</strong> <ul> <li><em>Relatively simple implementation</em>: Similar to random unstructured pruning, implementing \(\ell_1\)​ norm-based unstructured pruning is generally straightforward.</li> <li><em>Deterministic pruning</em>: Unlike random methods, \(\ell_1\)​ pruning is deterministic. Given the same network and pruning ratio, the same weights will always be pruned, making experiments reproducible.</li> <li><em>High flexibility</em>: It offers high flexibility by allowing the pruning of any individual weight within any layer of the network.</li> <li><em>Generally better accuracy than \(\ell_1\)​ structured pruning</em>: For a given sparsity level, \(\ell_1\)​ unstructured pruning typically achieves higher accuracy compared to its structured counterpart due to its fine-grained nature.</li> <li><em>Importance-driven pruning</em>: By focusing on pruning weights with the lowest \(\ell_1\)​ norm (which indicates lower magnitude and potentially less importance), this method can often achieve better performance than random unstructured pruning, especially at higher compression ratios, as it strategically removes less influential connections.</li> </ul> </li> <li><strong>Cons of L1 unstructured pruning</strong> <ul> <li><em>Non-Uniform sparsity across dimensions</em>: The pruning pattern is not guaranteed to be uniform across different dimensions within a layer’s weight tensor. This can potentially lead to imbalances in the importance of different features or connections.</li> <li><em>Limited hardware acceleration</em>: The irregular sparsity pattern resulting from unstructured pruning makes it very challenging to efficiently accelerate inference on standard hardware or specialized accelerators, which typically benefit from more structured sparsity.</li> <li><em>Potential for suboptimal networks at high sparsity</em>: At very high pruning ratios, even low-magnitude weights can be crucial. Removing too many based solely on magnitude can disrupt the network’s learned weight distribution and lead to performance degradation.</li> </ul> </li> </ul> <h4 id="l1-structured">L1 structured</h4> <p>Finally in \(\ell_{n}\) structured, we prune across the dimensions of our tensors, dropping weights with the lowest \(\ell_{n}\) norm. Like previously, we will use the \(\ell_1\) norm in order to calculate the importance.</p> <p>It is interesting to see what happens when we prune across the dimensions of a weight matrix. Let’s take as an example a Linear layer and see what happens for <code class="language-plaintext highlighter-rouge">dim=0</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim0.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Now let’s take the case where <code class="language-plaintext highlighter-rouge">dim=1</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim1.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Suppose that we have a N-layer network. We can see that in the first case where <code class="language-plaintext highlighter-rouge">dim=0</code> for each layer \(L\) we prune the neurons of the \(L\) layer, whereas if <code class="language-plaintext highlighter-rouge">dim=1</code> we prune the neurons of the \(L+1\) layer.</p> <ul> <li><strong>Pros of L1 structured pruning</strong> <ul> <li><em>Relatively simple implementation</em>: Similar to random structured pruning, implementing \(\ell_1\)​ norm-based structured pruning is generally straightforward.</li> <li><em>Deterministic pruning</em>: Unlike random structured methods, \(\ell_1\) structured pruning is deterministic. Given the same network and pruning criteria, the same structural units will always be pruned.</li> <li><em>Flexibility in pruning dimensions</em>: It allows for pruning along different dimensions of a tensor (e.g., rows, columns, channels, filters), enabling various forms of structured sparsity.</li> <li><em>Potential for easier acceleration</em>: By removing entire structural components, the resulting weight tensors have reduced dimensions, which can make them more amenable to basic hardware acceleration compared to unstructured sparsity.</li> <li><em>Quick path to model compression</em>: \(\ell_1\)​ structured pruning can provide a fast and effective way to obtain a compressed model with reduced size and computational cost.</li> <li><em>Achieves high compression ratios</em>: Due to the removal of entire structural units, this method can often achieve significant reductions in the number of parameters, leading to high compression ratios.</li> </ul> </li> <li><strong>Cons of L1 structured pruning</strong> <ul> <li><em>The effectiveness</em> of structured pruning can be highly dependent on the specific architecture of the neural network. Some architectures might be more sensitive to the removal of certain structural units than others.</li> <li><em>Lower flexibility</em> Compared to Unstructured Pruning: Pruning entire rows, columns, or other structural units is less flexible than removing individual weights, potentially limiting the ability to fine-tune the sparsity pattern for optimal accuracy retention.</li> <li><em>More aggressive accuracy reduction at high sparsity</em>: Compared to \(\ell_1\) unstructured pruning, \(\ell_1\) structured pruning tends to be more aggressive in reducing accuracy, especially at high pruning ratios, as the removal of entire structures can have a more significant impact on the network’s representational capacity.</li> <li><em>Difficulty in leveraging structure</em>: Again as previously, even though structured pruning offers us some structure in the resulting pruned layer, its difficult to drop the dimensions entirely and requires very careful stitching.</li> </ul> </li> </ul> <p>In the rest of the post, we will explore each of those techniques on the ImageNet validation dataset (50000) by using as a base model the Vision Transformer model that you can find <a href="https://huggingface.co/google/vit-base-patch16-224">[here]</a></p> <h3 id="visual-transformers-in-imagenet1000">Visual Transformers in ImageNet1000</h3> <p>Now that we’ve explored the different pruning methods, it’s time to see how they perform in practice. We evaluated these techniques on the challenging ImageNet dataset, using the popular Vision Transformer architecture as our foundation. In our experiments, we adopted a strategy of applying the same pruning ratio to each layer throughout the pruning process, with the exception of the <code class="language-plaintext highlighter-rouge">LayerNorm</code> layers.</p> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(2, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/imagenet1000/pruning_results_random_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_random_structured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_l1_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"/> <img src="/assets/post_images/imagenet1000/pruning_results_ln_structured.png" alt="Image 1" style="width: 100%; height: auto;"/> </div> <p>The results indicate that the <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning method yielded the most favorable trade-off between compression ratio and accuracy, which aligns with the expectation that removing weights with the lowest importance (as determined by the L1 norm) would be effective. As previously noted, the ln_structured method proved to be highly aggressive, leading to an immediate and significant drop in accuracy to an unacceptable level of approximately 20%.</p> <p>To mitigate the accuracy loss observed during pruning, we will investigate investigate in a bit the impact of fine-tuning. Specifically, we will attempt to recover accuracy for the <code class="language-plaintext highlighter-rouge">l1_unstructured</code> method at a pruning ratio of 0.5</p> <p>We can also analyze the impact of pruning and potential fine-tuning, we can visualize the weight distribution of the classifier layer by examining its histograms.</p> <h4 id="visualizing-weight-distributions-classifier-layer-across-the-different-methods">Visualizing Weight Distributions (Classifier layer) across the different methods</h4> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(3, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/cls_histogram_base.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/cls_histogram_l1_unstructured.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/cls_histogram_ln_structured.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/cls_histogram_random_structured.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/cls_histogram_random_unstructured.png" style="width: 100%; height: auto;"/> </div> <h4 id="sensitivity-analysis-for-l1-structured-further-examining-structured-pruning">Sensitivity analysis for L1 structured (further examining structured pruning)</h4> <p>Before we proceed to the last part of this post, which covers fine-tuning, we’ll first conduct a sensitivity analysis with \(\ell_1\)​ structured pruning. The goal is to see if we can gain better accuracy by carefully choosing which layers to prune. We’ll start by examining how the model performs when we try different pruning ratios for each layer.</p> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(2, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/l1_structured_cl_vs_projection.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/l1_structured_l11_layer.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/l1_structured_l0_layer.png" style="width: 100%; height: auto;"/> <img src="/assets/post_images/l1_structured_l5_layer.png" style="width: 100%; height: auto;"/> </div> <p>It’s interesting to observe that the effect of pruning varies significantly across different layers. Our results indicate that the embedding and classifier layers are the most important, which makes sense as they are the initial and final stages of the network. Heavily pruning these two layers can severely impact the network’s performance.</p> <p>Additionally, the first Attention layer appears to be quite sensitive, especially when its intermediate dense module is heavily pruned. On the other hand, the last Attention layer and the 6th Attention layer, which is around the middle, seem to be less affected by pruning, even with substantial pruning ratios.</p> <blockquote> <p>💡 <strong>Important note</strong> <br/><br/> It’s important to remember that while sensitivity analysis offers valuable insights into the pruneability of individual layers, it doesn’t account for the inter-dependencies between them.</p> </blockquote> <p>Based on our initial experiments, we first attempted to apply pruning to all layers except the 1st, 12th, the Embeddings Projection (Convolutional layer), and the Classifier layer. However, we observed that the inter-dependencies between layers significantly impacted the outcome, with even modest pruning of the Attention layers leading to a decline in performance.</p> <p>The most effective pruning configuration we identified involved applying \(\ell_1\)​ structured pruning to all layers except:</p> <ul> <li>The 1st layer</li> <li>The 12th layer</li> <li>The Embeddings Projection (Convolutional layer)</li> <li>The Classifier layer</li> </ul> <p>Furthermore, within the Attention blocks of the remaining layers, we specifically pruned only the intermediate and output tensors, while preserving the <em>query</em>, <em>key</em>, and <em>value</em> tensors.</p> <p>This configuration yielded an accuracy of 44% with a model sparsity of 20% in the \(\ell_1\) structured pruning. Our next step will be to investigate whether fine-tuning can help recover some of the accuracy lost during the pruning process.</p> <h3 id="finetuning-with-pruning">Finetuning with pruning</h3> <p>In a final effort to reclaim some of the performance lost during pruning, we proceeded to fine-tune the resulting sparse network. This step proved fruitful, allowing us to recover a significant portion of the original accuracy. Starting from an initial accuracy of 80.39%, we reached a respectable 67.24% with \(\ell_1\) structured after fine-tuning.</p> <p>This represents a recovery of approximately 83% of the accuracy lost due to pruning. Considering that our initial aggressive application of \(\ell_1\) structured pruning had severely impacted performance (dropping accuracy to around 20%, as noted earlier), the jump to 67.24% through careful layer selection and subsequent fine-tuning demonstrates the potential of this approach. This outcome suggests that while aggressive structured pruning can initially lead to a substantial accuracy drop, a well-executed fine-tuning strategy can effectively bridge the performance gap.</p> <blockquote> <p>💡 <strong>Important Consideration for Finetuning</strong> <br/><br/> In our earlier steps, we used the <code class="language-plaintext highlighter-rouge">remove()</code> function to solidify the weight mask and effectively eliminate pruned elements. <br/> <br/> This made the pruning permanent. However, when finetuning, we take a different approach. We deliberately don’t immediately make the pruned elements permanent. The reason for this is that we want the weight masks to remain active, allowing the backward pass to specifically focus on updating the weights of the unpruned elements.</p> </blockquote> <h3 id="iterative-approach">Iterative approach</h3> <p>So, building on what we’ve learned, another way to really optimize our model is to try an iterative approach – kind of like a cycle of pruning and fine-tuning.</p> <p>We can try snipping away a little bit of the network, and then giving it a quick tune-up to make sure it hasn’t lost its edge. By doing this over and over, we can often make the model even smaller and still keep it performing really well, sometimes even better than if we just pruned it all at once.</p> <p><br/></p> <div style="text-align: center;"> <img src="/assets/post_images/iterative_pruning.png" style="width: 60%;"/> </div> <p><br/></p> <p>This gradual process helps the network get used to having fewer connections and learn to work efficiently with what it has. Since we already saw some good results with just one round of pruning and tuning, giving this iterative method a shot looks like a promising next step to really make our Vision Transformer shine when we want to put it to work! It’s all about finding that sweet spot of size and performance.</p> <h3 id="results">Results</h3> <p>Now that we’ve covered the basics, let’s dive into the results! The figure below compares the performance of the various pruning methods and strategies we experimented with.</p> <p><br/></p> <div style="text-align: center;"> <img src="/assets/post_images/ln_structured_base_finetuned_iter.png" style="width: 100%;"/> </div> <ul> <li> <p>For the Random unstructured, Random structured, and L1 structured methods, we only achieved sensible results using a 22% pruning ratio, and only when restricting pruning to the layers mentioned previously.</p> </li> <li> <p>When it comes to preserving accuracy, L1 unstructured pruning proved to be the most effective approach. In fact, we were able to restore the network to its original level of performance using this method.</p> </li> </ul> <h3 id="in-conclusion-pruning-for-efficiency">In Conclusion: Pruning for Efficiency</h3> <p>Our exploration into post-training model optimization highlighted pruning as a valuable technique for creating more efficient machine learning models. We observed that L1 unstructured pruning excelled at maintaining accuracy, while structured methods offer potential speedups with careful application. Fine-tuning proved essential for recovering performance after pruning, and iterative approaches hold promise for further optimization.</p> <p>Ultimately, pruning is a key tool in the ongoing effort to deploy powerful AI models efficiently across various platforms.</p> <p>You can find the code available here: <a href="https://github.com/psouranis/pruning">Github</a></p>]]></content><author><name></name></author><category term="Optimizations"/><category term="Inference"/><category term="Optimizations"/><category term="Performance"/><category term="ML"/><summary type="html"><![CDATA[Pruning Techniques]]></summary></entry><entry><title type="html">Tail Call Optimization</title><link href="https://psouranis.github.io/blog/2025/tail-call-optimization/" rel="alternate" type="text/html" title="Tail Call Optimization"/><published>2025-02-14T12:40:16+00:00</published><updated>2025-02-14T12:40:16+00:00</updated><id>https://psouranis.github.io/blog/2025/tail-call-optimization</id><content type="html" xml:base="https://psouranis.github.io/blog/2025/tail-call-optimization/"><![CDATA[<h3 id="tail-recursive-methods">Tail Recursive Methods</h3> <p>In this post we will talk about optimizations in C and specifically about Tail Call Optimization. But let’s start from defining what is a tail recursion.</p> <p>A recursive method is referred to as tail-recursive <em>when the recursive call is the last thing executed by the that method</em>, otherwise, it’s known as <em>head recursive</em></p> <p>Here is a simple example of a <em>tail recursive method</em>:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">void</span> <span class="nf">printNumbers</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>	
	<span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
	<span class="n">printNumbers</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Notice in this function how the last thing executed by the method is a recrusive call, making it a <em>tail recursive method</em>. Let’s take another example now, the factorial function. The factorial function can be implemented as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Is this a tail recursive method? On the first look someone would say yes but if we were to look more closely we would see that the function can be written as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
	<span class="k">return</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x</span><span class="p">;</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>We realize that the last executed statement by this method is the multiplication and not the recursion meaning that the method we previously implemented is not <em>tail recursive</em> but rather <em>head recursive</em>. So let’s try first to transform it to a tail recursive method and then discuss it’s benefits.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">factorial</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">return</span> <span class="n">factorialTail</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">factorialTail</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">x</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">return</span> <span class="n">x</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="n">factorial</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span> <span class="o">*</span> <span class="n">x</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Notice that now we have 2 methods instead of 1 (<code class="language-plaintext highlighter-rouge">factorial</code> and <code class="language-plaintext highlighter-rouge">factorialTail</code>). The original method is turned to a helper method and is now called with an initial value. This initial value, in most cases, will be the return value of the stop condition we had in the original recursive call.</p> <p>The next thing you will notice, <em>is how we included the multiplication</em> which was the operation executed after the recursive call, inside the the recursion itself as an <strong>accumulator</strong> and it is the final result of this <strong>accumulator</strong> that is being returned to the user when the calculation is finished.</p> <p>In a tail recusrive method, <em>the result of the stop condition is actually the result of the whole recursion</em> because that is what it will be returned by all child calls all the way to the parent and initial call.</p> <p>Let’s take another example as well, the Fibonnaci sequence:</p> \[\text{Fibonacci}(n) = \text{Fibonnaci}(n-1) + \text{Fibonnaci}(n-2)\] <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">fibonnaci</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">fibonnaci</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">fibonnaci</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Can be transformed to a tail recursive method as follows:</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="kt">int</span> <span class="nf">fibonnaci</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">return</span> <span class="n">fibonnaciTail</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">fibonnaciTail</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="n">a</span><span class="p">;</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="n">b</span><span class="p">;</span>
	<span class="k">return</span> <span class="n">fibonnaci_</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">);</span>
<span class="p">}</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>Similar to the factorial one, the original method is broken into two, the caller and the helper one. The caller method is using as initial values the return values of the stop-conditions we previously had. The result or the addition is now accumulated inside the recursive call and because the recursion is called on the previous value of our current value as well in the initial method – we need to keep track the previous value of the initial call using an additional parameter.</p> <h3 id="continuation-passing-style">Continuation Passing Style</h3> <ol> <li> <p>The continuation passing style (passing accumulators as parameter values) might be the <strong>only generic way</strong> that allows you to transform the method into a form that uses <strong>only-tail calls</strong>.</p> </li> <li> <p>Not all logics can be made tail-recursive, non-linear recursions for instance maintain a variable somewhere and that somewhere is the <strong>Stack Memory</strong>.</p> </li> </ol> <h3 id="stack-visualization">Stack Visualization</h3> <p>Take a look at how the stack frame with tail call optimization and without.</p> <div style="text-align: center;"> <img src="/assets/post_images/tco.png"/> </div> <p>When <code class="language-plaintext highlighter-rouge">factorial(5)</code> let’s say is called without tail call optimization — these calls will be respectively added to the stack, and we won’t be able to start popping from this stack before we reach the stop condition of the recursion.</p> <p>When that happens the values we have will start to get replaced and the stack calls will be popped.</p> \[\text{factorial}(5) \rightarrow \text{factorial}(4) \rightarrow \text{factorial}(3) \rightarrow \text{factorial}(2) \rightarrow \text{factorial}(1)\] <p>However, if we try this on the tail recursive method, the first call will be the call to the factorial method <code class="language-plaintext highlighter-rouge">factorial</code> and the second call will be the call to the <code class="language-plaintext highlighter-rouge">factorialTail</code>:</p> \[\text{factorial}(5) \rightarrow \text{factorialTail}(5,1)\] <p>When <code class="language-plaintext highlighter-rouge">factorial(4)</code> is called, because no variables are required to be stored in the frame left by the parent call of the same recursive method, and because both of these calls will have the same return value — then this same frame will be used to store the new call details:</p> \[\text{factorial}(5) \rightarrow \text{factorialTail}(1, 120)\] <p>Now, if we go back and compare the stacks corresponding to the recursive methods at their maximum depths, we will see that the space complexity of recursion was reduced from \(O(n)\) to \(O(1)\) thanks to tail-call elimination.</p> <div style="text-align: center;"> <img src="/assets/post_images/tco_complexity.png" style="width: 50%;"/> </div> <h3 id="summary">Summary</h3> <h4 id="what-are-the-benefits-ot-tail-recursion-methods">What are the benefits ot Tail Recursion Methods?</h4> <ul> <li>Tail recursive methods are <strong>optimized</strong> by certain compilers. These compilers usually execute calls with the help of a stack.</li> <li>In case of tail-recursive methods, there is no need to reserve a place in the stack for the recursive call because there is nothing left to do in the current method and we won’t be returning to the parent call.</li> <li>The space complexity reduces from \(O(n)\) to \(O(1).\)</li> </ul>]]></content><author><name></name></author><category term="Optimizations"/><category term="C"/><category term="Optimizations"/><category term="Performance"/><summary type="html"><![CDATA[Tail Call Optimization]]></summary></entry></feed>