---
layout: post
title: __getitem__ vs __getitems__. A nice trick for Pytorch
date: 2025-15-05 12:40:16
description: Torch indexing trick
tags: Inference Optimizations Performance ML
categories: Optimizations
thumbnail: assets/post_images/getitem/__getitem__.jpeg
giscus_comments: true
toc: 
  beginning: true
---

<br>
<div style="text-align: center;">
  <img src="/assets/post_images/getitem/__getitem__.jpeg" style="width: 50%; height: auto;">
</div>
<br>


This post is about one curious thing we discovered when we were dealing Pytorch's `DataLoader`. Imagine the following scenario.
Let's say that you want to train an LSTM model for example on some embeddings. What we would have would be the following:

$$
\text{x\_sample} = (x_{0}, x_{1}, \ldots, x_{k}), \text{where }  k=\text{window\_size} \text{ and } x_{n} \in \mathbf{R}^{d} \text{ where } d=\text{embedding\_dimension}
$$ 

In Python we could represent it very easily like this:

```python
import numpy as np
from numpy.lib.stride_tricks import sliding_window_view
window_shape = 10 # How long our window will be

windows = sliding_window_view(np.arange(100), window_shape=window_shape)
embeddings = np.random.randn(100, 384)

print(windows.shape)
# (91, 10)
print(embeddings.shape)
# (100, 384)
print(windows[0:2, :])
#[[ 0  1  2  3  4  5  6  7  8  9]
# [ 1  2  3  4  5  6  7  8  9 10]]
```

Imagine as a concrete example that each index can be a word and each word has its unique embedding.

> *note*
>
> We are not creating an entire dataset of shape (91, 10, 384) because we want to cache the embeddings and only index
> them when we are retrieving an example in order to avoid to be out of memory. This way we have a total memory consuption of
> 91 * 10 * 2 = 1820 and 384 * 100 * 8 = 3072, 309020 bytes so roughly 309 MBs. Instead if we create the whole embedding dataset we would have 91 * 10 * 384 * 8 = 2795520 bytes so roughly 3 GBs.  


Now let's say that we want to create a Torch `Dataset` in order to retrieve samples from this training set. What we would normally do is the following:

```python
import torch
import torch.nn as nn
from torch import Tensor
from torch.utils.data import Dataset

class WindowEmbeddingDataset(Dataset):
  def __init__(self, embeddings: nn.Embedding, windows: Tensor):
    self.embeddings = embeddings
    self.windows = windows

  def __getitem__(self,idx):
    return self.embeddings(self.windows[idx])
  
  def __len__(self):
    return len(self.windows)

nn_embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings).view(-1,1))
dataset = WindowEmbeddingDataset(nn_embedding, torch.tensor(windows))

print(dataset[0].shape)
# torch.Size([10, 384])
```

Alright so far so good, so now our next step would be to create a `DataLoader`.

```python
from torch.utils.data import DataLoader
dl = DataLoader(dataset, batch_size=8)
```

Let's see how fast we can iterate through our `Dataset` with our `DataLoader`.

```python
import time
start = time.time()
for _ in dl:
    pass
print(time.time() - start)
# 0.0038449764251708984
```
Now as you can see in order to iterate through our whole dataset which consists of 91 examples, we needed 
with batch size of 8 samples around 3 miliseconds. That's quite fast but let's see if we can do anything better.

If you check the documentation and the source code of the `Dataset` class you will end up to the class `_MapDatasetFetcher`.

```python
class _MapDatasetFetcher(_BaseDatasetFetcher):
    def fetch(self, possibly_batched_index):
        if self.auto_collation:
            if hasattr(self.dataset, "__getitems__") and self.dataset.__getitems__:
                data = self.dataset.__getitems__(possibly_batched_index)
            else:
                data = [self.dataset[idx] for idx in possibly_batched_index]
        else:
            data = self.dataset[possibly_batched_index]
        return self.collate_fn(data)
```

You can spot the devil in the details right? 

```python
data = [self.dataset[idx] for idx in possibly_batched_index]
```

This line right here, means that our Dataset collects each window embedding seperately and then it concats them with the help of the `self.collate_fn` function.
So this is all fine and good but can't we collect all the indexes at once and make a single call to `__getitem__` ? After all the `nn.Embedding` supports all sorts of types
of indexing and its optimized at its core to make such calls.

This is where `__getitems__` comes in play. This little function which for some reason is not documented in Pytorch's documentation is extremely helpful and serves exactly this purpose.
Instead of making multiple calls to `__getitem__` and stack them in `collate_fn` we will make a single call to `__getitems__` after we have collected all indexes. And to implement it is very straightforward.

```python
import torch
import torch.nn as nn
from torch import Tensor
from torch.utils.data import Dataset

class WindowEmbeddingDataset(Dataset):
  def __init__(self, embeddings: nn.Embedding, windows: Tensor):
    self.embeddings = embeddings
    self.windows = windows

  def __getitems__(self,idxs):
    return self.embeddings(self.windows[idxs])
  
  def __len__(self):
    return len(self.windows)

nn_embedding = nn.Embedding.from_pretrained(torch.tensor(embeddings).view(-1,1))
dataset = WindowEmbeddingDataset(nn_embedding, torch.tensor(windows))
```
What we did is to replace the `__getitem__` with the `__getitems__`. Now let's see if we can have any improvement.

```python
from torch.utils.data import DataLoader
dl = DataLoader(dataset, batch_size=8, collate_fn=lambda x: x)

import time
start = time.time()
for _ in dl:
    pass
print(time.time() - start)
# 0.0024793148040771484
```
As you will notice here, we also replaced the default collate function which uses `torch.stack` to collect all the examples with just
`collate_fn=lambda x: x` since each example now is a batch itself!.

The difference as you can see is minor here but the results become more apparent once we increase the size of our window dataset. Let's do that and see the difference

```python
windows = np.repeat(windows, repeats=1000, axis=0)
```
Here, we just repeated the rows.

Now, with our first version, we get the following timing.

- `__getitem__`: 1.2 seconds.
- `__getitems__`: 0.6 seconds.

So we are down by 50% by making a pretty small change in our code. So next time you will implement an Embedding dataset just remember that 
the devil is in the details and hidden `concats` can make a great difference in the performance.




