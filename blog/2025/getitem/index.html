<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> __getitem__ vs __getitems__. A nice trick for Pytorch | Panagiotis Souranis </title> <meta name="author" content="Panagiotis Souranis"> <meta name="description" content="Torch indexing trick"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?e085e6e336e4118fa5acc3f249df0647" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://psouranis.github.io/blog/2025/getitem/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Panagiotis</span> Souranis </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">__getitem__ vs __getitems__. A nice trick for Pytorch</h1> <p class="post-meta"> Created in May 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Inference</a>   <a href="/blog/tag/optimizations"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimizations</a>   <a href="/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/optimizations"> <i class="fa-solid fa-tag fa-sm"></i> Optimizations</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> </ul> </div> <hr> <div id="markdown-content"> <p><br></p> <div style="text-align: center;"> <img src="/assets/post_images/getitem/getitem.png" style="width: 50%; height: auto;"> </div> <p><br></p> <p>This post is about one interesting thing we discovered when we were dealing Pytorch’s <code class="language-plaintext highlighter-rouge">DataLoader</code>. Imagine the following scenario. Let’s say that you want to train an LSTM model for example on some embeddings. What we would have would be the following:</p> \[x_\text{sample} = (x_{0}, x_{1}, \ldots, x_{k}), \text{where } k=\text{window size} \text{ and } x_{i} \in \mathbf{R}^{d} \text{ where } d=\text{embedding dimension}\] <p>In Python we could represent it very easily like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">numpy.lib.stride_tricks</span> <span class="kn">import</span> <span class="n">sliding_window_view</span>
<span class="n">window_shape</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># How long our window will be
</span>
<span class="n">windows</span> <span class="o">=</span> <span class="nf">sliding_window_view</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span> <span class="n">window_shape</span><span class="o">=</span><span class="n">window_shape</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">384</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">windows</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (91, 10)
</span><span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># (100, 384)
</span><span class="nf">print</span><span class="p">(</span><span class="n">windows</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="c1">#[[ 0  1  2  3  4  5  6  7  8  9]
# [ 1  2  3  4  5  6  7  8  9 10]]
</span></pre></td> </tr></tbody></table></code></pre></div></div> <p>Imagine as a concrete example that each index can be a word and each word has its unique embedding.</p> <blockquote> <p><em>note</em></p> <p>We are not creating an entire dataset of shape (91, 10, 384) because we want to cache the embeddings and only index them when we are retrieving an example in order to avoid to be out of memory.</p> <p>This way we have a total memory consuption of 91 * 10 * 2 = 1820 and 384 * 100 * 8 = 3072, 309020 bytes so roughly 309 MBs. Instead if we create the whole embedding dataset we would have 91 * 10 * 384 * 8 = 2795520 bytes so roughly 3 GBs.</p> </blockquote> <p>Now let’s say that we want to create a Torch <code class="language-plaintext highlighter-rouge">Dataset</code> in order to retrieve samples from this training set. What we would normally do is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">windows</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span>
    <span class="n">self</span><span class="p">.</span><span class="n">windows</span> <span class="o">=</span> <span class="n">windows</span>

  <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">idx</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">)</span>

<span class="n">nn_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">nn_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">windows</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># torch.Size([10, 384])
</span></pre></td> </tr></tbody></table></code></pre></div></div> <p>Alright so far so good, so now our next step would be to create a <code class="language-plaintext highlighter-rouge">DataLoader</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>Let’s see how fast we can iterate through our <code class="language-plaintext highlighter-rouge">Dataset</code> with our <code class="language-plaintext highlighter-rouge">DataLoader</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">time</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="nf">print</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
<span class="c1"># 0.0038449764251708984
</span></pre></td> </tr></tbody></table></code></pre></div></div> <p>Now as you can see in order to iterate through our whole dataset which consists of 91 examples, we needed with batch size of 8 samples around 3 miliseconds. That’s quite fast but let’s see if we can do anything better.</p> <p>If you check the documentation and the source code of the <code class="language-plaintext highlighter-rouge">Dataset</code> class you will end up to the class <code class="language-plaintext highlighter-rouge">_MapDatasetFetcher</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td> <td class="rouge-code"><pre><span class="k">class</span> <span class="nc">_MapDatasetFetcher</span><span class="p">(</span><span class="n">_BaseDatasetFetcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">possibly_batched_index</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">auto_collation</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">,</span> <span class="sh">"</span><span class="s">__getitems__</span><span class="sh">"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="n">__getitems__</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">.</span><span class="nf">__getitems__</span><span class="p">(</span><span class="n">possibly_batched_index</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">possibly_batched_index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>You can spot the devil in the details right?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">possibly_batched_index</span><span class="p">]</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>This line right here, means that our <code class="language-plaintext highlighter-rouge">Dataset</code> collects each window embedding seperately and then it concats them with the help of the <code class="language-plaintext highlighter-rouge">self.collate_fn</code> function. So this is all fine and good but can’t we collect all the indexes at once and make a single call to <code class="language-plaintext highlighter-rouge">__getitem__</code> ? After all the <code class="language-plaintext highlighter-rouge">nn.Embedding</code> supports all sorts of types of indexing and its optimized at its core to make such calls.</p> <p>This is where <code class="language-plaintext highlighter-rouge">__getitems__</code> comes in play. This little function which for some reason is not documented in Pytorch’s documentation is extremely helpful and serves exactly this purpose. Instead of making multiple calls to <code class="language-plaintext highlighter-rouge">__getitem__</code> and stack them in <code class="language-plaintext highlighter-rouge">collate_fn</code> we will make a single call to <code class="language-plaintext highlighter-rouge">__getitems__</code> after we have collected all indexes. And to implement it is very straightforward.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">,</span> <span class="n">windows</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span>
    <span class="n">self</span><span class="p">.</span><span class="n">windows</span> <span class="o">=</span> <span class="n">windows</span>

  <span class="k">def</span> <span class="nf">__getitems__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">idxs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">embeddings</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">[</span><span class="n">idxs</span><span class="p">])</span>
  
  <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">windows</span><span class="p">)</span>

<span class="n">nn_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nc">WindowEmbeddingDataset</span><span class="p">(</span><span class="n">nn_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">windows</span><span class="p">))</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>What we did is to replace the <code class="language-plaintext highlighter-rouge">__getitem__</code> with the <code class="language-plaintext highlighter-rouge">__getitems__</code>. Now let’s see if we can have any improvement.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">dl</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

<span class="kn">import</span> <span class="n">time</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
    <span class="k">pass</span>
<span class="nf">print</span><span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
<span class="c1"># 0.0024793148040771484
</span></pre></td> </tr></tbody></table></code></pre></div></div> <p>As you will notice here, we also replaced the default collate function which uses <code class="language-plaintext highlighter-rouge">torch.stack</code> to collect all the examples with just <code class="language-plaintext highlighter-rouge">collate_fn=lambda x: x</code> since each example now is a batch itself!.</p> <p>The difference as you can see is minor here but the results become more apparent once we increase the size of our window dataset and once you start increasing the batch size (more calls on <code class="language-plaintext highlighter-rouge">__getitem__</code>). Let’s do that and see the difference</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre><span class="n">windows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">windows</span><span class="p">,</span> <span class="n">repeats</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>Here, we just repeated the rows.</p> <p>Now, with our first version, we get the following timing.</p> <ul> <li> <code class="language-plaintext highlighter-rouge">__getitem__</code>: <em>1.2</em> seconds.</li> </ul> <p>With our second version we have:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">__getitems__</code>: <em>0.6</em> seconds.</li> </ul> <p>So we are down by 50% by making a pretty small change in our code. So next time you will implement an Embedding dataset just remember that the devil is in the details and hidden <code class="language-plaintext highlighter-rouge">concats</code> can make a great difference in the performance.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/pruning-techniques-part2/">Pruning techniques - Optimizing machine learning models Part 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/pruning-techniques/">Pruning techniques - Optimizing machine learning models Part 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tail-call-optimization/">Tail Call Optimization</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"psouranis/psouranis.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Panagiotis Souranis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-curriculum-vitae",title:"Curriculum Vitae",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-getitem-vs-getitems-a-nice-trick-for-pytorch",title:"__getitem__ vs __getitems__. A nice trick for Pytorch",description:"Torch indexing trick",section:"Posts",handler:()=>{window.location.href="/blog/2025/getitem/"}},{id:"post-pruning-techniques-optimizing-machine-learning-models-part-2",title:"Pruning techniques - Optimizing machine learning models Part 2",description:"Pruning Techniques Part 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/pruning-techniques-part2/"}},{id:"post-pruning-techniques-optimizing-machine-learning-models-part-1",title:"Pruning techniques - Optimizing machine learning models Part 1",description:"Pruning Techniques",section:"Posts",handler:()=>{window.location.href="/blog/2025/pruning-techniques/"}},{id:"post-tail-call-optimization",title:"Tail Call Optimization",description:"Tail Call Optimization",section:"Posts",handler:()=>{window.location.href="/blog/2025/tail-call-optimization/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%6F%75%72%61%6E%69%73%70%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>