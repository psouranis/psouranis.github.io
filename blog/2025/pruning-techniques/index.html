<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pruning techniques - Optimizing machine learning models Part 1 | Panagiotis Souranis </title> <meta name="author" content="Panagiotis Souranis"> <meta name="description" content="Pruning Techniques"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?ff865bd8fdca0e528cc13dbdeaa7bb43" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://psouranis.github.io/blog/2025/pruning-techniques/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?ed8976c660d4744649845ea229e8b218" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Panagiotis</span> Souranis </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pruning techniques - Optimizing machine learning models Part 1</h1> <p class="post-meta"> Created in March 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Inference</a>   <a href="/blog/tag/optimizations"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimizations</a>   <a href="/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/optimizations"> <i class="fa-solid fa-tag fa-sm"></i> Optimizations</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"> <a href="#pruning">Pruning</a> <ul> <li class="toc-entry toc-h4"><a href="#global-vs-local-pruning">Global vs Local pruning</a></li> <li class="toc-entry toc-h4"><a href="#random-unstructured">Random Unstructured</a></li> <li class="toc-entry toc-h4"><a href="#random-structured">Random Structured</a></li> <li class="toc-entry toc-h4"><a href="#ln-unstructured">Ln Unstructured</a></li> <li class="toc-entry toc-h4"><a href="#ln-structured">Ln Structured</a></li> <li class="toc-entry toc-h4"><a href="#results-in-imagenet1000-with-vit">Results in ImageNet1000 with VIT</a></li> <li class="toc-entry toc-h4"><a href="#finetuning-with-pruning">Finetuning with pruning</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>In this article, we are going to focus in optimizing machine learning models after training ends.</p> <h3 id="pruning">Pruning</h3> <div style="text-align: center;"> <img src="/assets/post_images/pruned_tree.png"> </div> <p>Pruning is the practice of removing parameters (which may entail removing individual parameters, or parameters in groups such as by neurons) from an existing artificial neural networks.[1] The goal of this process is to maintain accuracy of the network while increasing its efficiency. This can be done to reduce the computational resources required to run the neural network <a href="https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)" rel="external nofollow noopener" target="_blank">[1]</a>.</p> <div style="text-align: center;"> <img src="/assets/post_images/pruned_network.png"> </div> <p>There are 2 distinctions in pruning: <strong>Global</strong> and <strong>Local</strong> pruning.</p> <h4 id="global-vs-local-pruning">Global vs Local pruning</h4> <p>The difference between global and local pruning lies in whether structures are removed from a subset or all available structures of a network. A major limitation of local pruning is that setting a pre-defined prune ratio for each layer can be complex and lead to sub-optimal sparsity. To simplify, local pruning often uses a consistent prune ratio across layers. In contrast, global pruning automatically generates a varying prune ratio for each layer. However, global pruning poses great challenges (particularly for LLMs), due to significant variations in layer magnitudes. For instance, some outlier features may have magnitudes up to 20 times larger than others, leading to incomparability issues.</p> <p>In the following post, we are going to focus on <strong>Local</strong> pruning since it poses an advantage compared to global pruning.</p> <p>Let’s review some of the most common techniques for pruning:</p> <h4 id="random-unstructured">Random Unstructured</h4> <p>As the name suggest, in random unstructured pruning we prune each tensor of our network by randomly removing weights (connections between neurons) of its layers. In PyTorch the most common way to apply random unstructured pruning is via the <code class="language-plaintext highlighter-rouge">torch.nn.utils.random_unstrucuted</code>.</p> <p>We can apply it to each layer of our network like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_unstructured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_unstructured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td> </tr></tbody></table></code></pre></div></div> <p>In the script above there are couple of things happening:</p> <p>First, we iterate through each layer of our network using the <code class="language-plaintext highlighter-rouge">.named_modules()</code> method. Then we pass to the <code class="language-plaintext highlighter-rouge">random_unstructured</code> function the <code class="language-plaintext highlighter-rouge">module</code>, the <code class="language-plaintext highlighter-rouge">name</code> and the <code class="language-plaintext highlighter-rouge">amount</code> parameters. We do that because we want to target the <code class="language-plaintext highlighter-rouge">weight</code> attribute of our <code class="language-plaintext highlighter-rouge">Module</code> which contains the weights (it could also be applied to <code class="language-plaintext highlighter-rouge">bias</code> as well).</p> <p>Finally, we apply the <code class="language-plaintext highlighter-rouge">remove</code> method to the <code class="language-plaintext highlighter-rouge">module</code> because the <code class="language-plaintext highlighter-rouge">random_unstructured</code> method applies pruning to each tensor but also keeps the original weights in the attribute <code class="language-plaintext highlighter-rouge">weight_orig</code>. If we want to make the pruning permanent and discard the original weights, we have to remove them by using the <code class="language-plaintext highlighter-rouge">remove</code> function as we did above.</p> <blockquote> <p>💡 <strong>Important note</strong> <br><br> When we apply pruning through this method, essentially what we are doing is applying a random mask of zeroes and ones on every layer in order to cancel them out. You can actually access those masks via the <code class="language-plaintext highlighter-rouge">weight_mask</code> attribute. <br><br> It is important to make this clear since masking a layer does nothing to the network parameters and hence its size or to how many operations does. <br><br> The total number of parameters remains the same (it actually increases because we have to also store the masks along with the actual weights) but we will see later how we can actually prune the layer and indeed remove parameters from it.</p> </blockquote> <ul> <li> <em>Pros of Random Unstructured Pruning</em> <ul> <li>Simple to implement.</li> <li>Extremely flexible, as it can prune any element in any dimension.</li> <li>Generally has better performance in terms of accuracy. <br><br> </li> </ul> </li> <li> <em>Cons of Random Unstructured Pruning</em> <ul> <li>Pruning is not guaranteed to be uniform across the tensor.</li> <li>Pruning is not guaranteed to be uniform across the dimensions.</li> <li>Very hard to accelerate with hardware acceleration.</li> </ul> </li> </ul> <h4 id="random-structured">Random Structured</h4> <p>Structured pruning is very similar with unstructured pruning only in this case we don’t only remove weights (like previously) but we remove entire neurons (or entire structural components like filters, channels in Convolutional networks). This type of pruning tends to reduce accuracy of the model but it has the benefit that it achieves better performance in terms of inference speed. In this case by cutting entire rows or columns of our tensors we dramatically reduce the number of calculations that need to be executed.</p> <div style="text-align: center;"> <img src="/assets/post_images/structured_2d_pruning.png" style="max-width: 50%"> </div> <p>In PyTorch to apply random structured pruning, we can do so via the <code class="language-plaintext highlighter-rouge">torch.nn.utils.random_structured</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td> <td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torch.nn.utils</span> <span class="kn">import</span> <span class="n">random_structured</span><span class="p">,</span> <span class="n">remove</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">OurAwesomeNetwork</span><span class="p">()</span> <span class="c1"># nn.Module()
</span><span class="n">pruning_ratio</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Remove half of neurons in every tensor
</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
	<span class="nf">random_structured</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">,</span> <span class="n">amount</span><span class="o">=</span><span class="n">pruning_ratio</span><span class="p">)</span>
	<span class="nf">remove</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">weight</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># The parameter named name+'_orig' is removed from the parameter list.
</span></pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li> <em>Pros of Random Structured Pruning</em> <ul> <li>Simple to implement.</li> <li>Can be used to prune any dimension of a tensor.</li> <li>Easier to accelerate (just a smaller matrix)</li> <li>Generally it offers lower accuracy but better performance in terms of inference speed. <br><br> </li> </ul> </li> <li> <em>Cons of Random Sstructured Pruning</em> <ul> <li>Less flexible as it prunes entire rows or columns of tensors</li> <li>Pruning is not guaranteed to be uniform across the tensor.</li> </ul> </li> </ul> <h4 id="ln-unstructured">Ln Unstructured</h4> <p>In \(\ell_{n}\) unstructured, we prune each tensor of our network by removing the lowest weights according to the \(\ell_{n}\) norm. Let’s see the following example using the \(\ell_1\) norm to calculate the importance of the tensor weights:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_unstructured.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>We start with a <code class="language-plaintext highlighter-rouge">torch.Tensor</code> that has no pruned weights. We then apply <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning to prune the lowest weights. To do so we first calculate the \(\ell_{1}\) norm of the tensor:</p> \[\ell_{1}(t) = \sum_{i=1}^{n} |w_{i}|\] <p>Where \(n\) is the total number of elements in the Tensor. Then we remove the \(40\%\) of the lowest weights of our Tensor.</p> <p>Similar with random unstructured pruning, \(\ell_n\) unstructured purning shares the same pros and cons.</p> <ul> <li> <em>Pros of Ln Unstructured Pruning</em> <ul> <li>Simple to implement.</li> <li>Extremely flexible, as it can prune any element in any dimension.</li> <li>Generally has better performance in terms of accuracy compared to Ln Structured Pruning.</li> <li>Focuses more in the importance of weights compared to Random Unstructured and hence can result in higher performance compared to Random Unstructured with higher compression ratio. <br><br> </li> </ul> </li> <li> <em>Cons of Ln Unstructured Pruning</em> <ul> <li>Pruning is not guaranteed to be uniform across the dimensions.</li> <li>Very hard to accelerate with hardware acceleration.</li> <li>As the pruning ratio increases, it can result in suboptimal networks since it focuses more on the weights with lower importance and the distribution of weights will change dramatically.</li> </ul> </li> </ul> <h4 id="ln-structured">Ln Structured</h4> <p>Finally in \(\ell_{n}\) structured, we prune across the dimensions of our tensors, dropping weights with the lowest \(\ell_{n}\) norm. Like previously, we will use the \(\ell_1\) norm in order to calculate the importance.</p> <p>It is interesting to see what happens when we prune across the dimensions of a weight matrix. Let’s take as an example a Linear layer and see what happens for <code class="language-plaintext highlighter-rouge">dim=0</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim0.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Now let’s take the case where <code class="language-plaintext highlighter-rouge">dim=1</code>.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/l1_structured_dim1.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Suppose that we have a N-layer network. We can see that in the first case where <code class="language-plaintext highlighter-rouge">dim=0</code> for each layer \(L\) we prune the neurons of the \(L\) layer, whereas if <code class="language-plaintext highlighter-rouge">dim=1</code> we prune the neurons of the \(L+1\) layer.</p> <ul> <li> <em>Pros of Ln Structured Pruning</em> <ul> <li>Simple to implement.</li> <li>Can be used to prune any dimension of a tensor.</li> <li>Easier to accelerate (just a smaller matrix).</li> <li>It can provide easy quick wins to get a compressed model.</li> <li>Provides very high compression ratio. <br><br> </li> </ul> </li> <li> <em>Cons of Ln Structured Pruning</em> <ul> <li>Less flexible as it prunes entire rows or columns of tensors.</li> <li>Pruning is not guaranteed to be uniform across the tensor.</li> <li>It is more aggressive than Ln Unstructured and should be not used with high pruning ratios.</li> </ul> </li> </ul> <p>In the rest of the post, we will explore each of those techniques on the ImageNet validation dataset (50000) by using as a base model the Vision Transformer model that you can find <a href="https://huggingface.co/google/vit-base-patch16-224" rel="external nofollow noopener" target="_blank">[here]</a></p> <h4 id="results-in-imagenet1000-with-vit">Results in ImageNet1000 with VIT</h4> <p>After all is said and done, its now time to evaluate the different methods described above in the ImageNet dataset using as our base model the VisionTransformer. You can find the code that was used to run those experiments here: <a href="https://github.com/psouranis/pruning" rel="external nofollow noopener" target="_blank">link</a>.</p> <p>We pruned each layer of the network equally (with the same pruning ratio in each iteration) except the <code class="language-plaintext highlighter-rouge">LayerNorm</code> layers.</p> <div style="display: grid; grid-template-columns: repeat(2, 1fr); grid-template-rows: repeat(2, 1fr); gap: 10px; width: fit-content;"> <img src="/assets/post_images/imagenet1000/pruning_results_random_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"> <img src="/assets/post_images/imagenet1000/pruning_results_random_structured.png" alt="Image 1" style="width: 100%; height: auto;"> <img src="/assets/post_images/imagenet1000/pruning_results_l1_unstructured.png" alt="Image 1" style="width: 100%; height: auto;"> <img src="/assets/post_images/imagenet1000/pruning_results_ln_structured.png" alt="Image 1" style="width: 100%; height: auto;"> </div> <p>As we can see from the results above, our best compression-ratio / accuracy comes from the <code class="language-plaintext highlighter-rouge">l1_unstructured</code> method which is somewhat expected since it removes weights with the lowest importance. As we also mentioned above, <code class="language-plaintext highlighter-rouge">ln_structured</code> is extremely aggressive and immediatelly drives our accuracy close to 20% which is not acceptable.</p> <p>Finally we can try to recover some of our lost accuracy by finetuning our model. Let’s see if we can recover the accuracy for the case where pruning ratio is 0.5 and for <code class="language-plaintext highlighter-rouge">l1_unstructured</code> pruning method.</p> <h4 id="finetuning-with-pruning">Finetuning with pruning</h4> <p>In our experiments, we finetuned the pruned model in order to achieve some of the lost accuracy (around 71%). So, we managed to reduce our model size by 50% and lose only 12.5% of its initial accuracy.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tail-call-optimization/">Tail Call Optimization</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"psouranis/psouranis.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Panagiotis Souranis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-curriculum-vitae",title:"Curriculum Vitae",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-pruning-techniques-optimizing-machine-learning-models-part-1",title:"Pruning techniques - Optimizing machine learning models Part 1",description:"Pruning Techniques",section:"Posts",handler:()=>{window.location.href="/blog/2025/pruning-techniques/"}},{id:"post-tail-call-optimization",title:"Tail Call Optimization",description:"Tail Call Optimization",section:"Posts",handler:()=>{window.location.href="/blog/2025/tail-call-optimization/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%6F%75%72%61%6E%69%73%70%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>